{"cells":[{"cell_type":"markdown","metadata":{"id":"2t637KoTObRl"},"source":["### Data Loading"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/My Drive/TransR/Data/Raw/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QrB01ixePYe_","executionInfo":{"status":"ok","timestamp":1701867723445,"user_tz":-480,"elapsed":19837,"user":{"displayName":"Pengxin Wang","userId":"11146925416336724082"}},"outputId":"c1eb5b8f-dc56-41f2-9095-6ed1861e7962"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/TransR/Data/Raw\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"uT9QmojpObRo","executionInfo":{"status":"ok","timestamp":1701867730971,"user_tz":-480,"elapsed":3773,"user":{"displayName":"Pengxin Wang","userId":"11146925416336724082"}}},"outputs":[],"source":["import pandas as pd\n","\n","# these two file is just for pairing id with name\n","artists = pd.read_csv('./lastfm_raw/artists.dat', sep='\\t')\n","tags = pd.read_csv('./lastfm_raw/tags.dat', sep='\\t', encoding='ISO-8859-1')\n","\n","# these 3 files describe 3 different type of relationships\n","user_artists = pd.read_csv('./lastfm_raw/user_artists.dat', sep='\\t')\n","user_taggedartists = pd.read_csv('./lastfm_raw/user_taggedartists.dat', sep='\\t')\n","user_friends = pd.read_csv('./lastfm_raw/user_friends.dat', sep='\\t')"]},{"cell_type":"markdown","metadata":{"id":"qWBzVAXMObRp"},"source":["### Remove unnecessary features"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"vqk-ySNyObRp","executionInfo":{"status":"ok","timestamp":1701867735946,"user_tz":-480,"elapsed":311,"user":{"displayName":"Pengxin Wang","userId":"11146925416336724082"}}},"outputs":[],"source":["artists.drop(['url', 'pictureURL'], axis=1, inplace=True)\n","user_artists.drop(['weight'], axis=1, inplace=True)\n","user_taggedartists.drop(['day', 'month', 'year'], axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"BFxim05WObRq"},"source":["### Data Cleaning"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"k40DAkbsObRq","executionInfo":{"status":"ok","timestamp":1701867740052,"user_tz":-480,"elapsed":319,"user":{"displayName":"Pengxin Wang","userId":"11146925416336724082"}}},"outputs":[],"source":["# Clean 'user_artists' to ensure that it only contains valid 'artistID's\n","user_artists = user_artists[user_artists['artistID'].isin(artists['id'])]\n","\n","# Clean 'user_taggedartists' to ensure it only contains valid 'artistID's and 'tagID's\n","user_taggedartists = user_taggedartists[\n","    user_taggedartists['artistID'].isin(artists['id']) &\n","    user_taggedartists['tagID'].isin(tags['tagID'])\n","]\n","# Now user_artists and user_taggedartists should only contain IDs that are present in artists and tags respectively"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mh7HVMiVObRq","executionInfo":{"status":"ok","timestamp":1701867800501,"user_tz":-480,"elapsed":378,"user":{"displayName":"Pengxin Wang","userId":"11146925416336724082"}},"outputId":"abb72048-7049-4f93-d7b8-7c0f78270131"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of unique users: 1892\n","Number of 'friend' relation pairs: 25434\n","Number of unique artists: 17632\n","Number of unique tags: 9718\n","Number of 'listens_to' relations: 92834\n","Number of 'assigns_tag' relations: 35730\n","Number of 'has_tag' relations: 108437\n"]}],"source":["# Count unique users\n","unique_users = pd.unique(pd.concat([user_artists['userID'], user_friends['userID']]))\n","num_unique_users = len(unique_users)\n","print(f\"Number of unique users: {num_unique_users}\")\n","\n","# Count friend relation pairs\n","num_friend_pairs = len(user_friends)\n","print(f\"Number of 'friend' relation pairs: {num_friend_pairs}\")\n","\n","# Count unique artists\n","unique_artists = pd.unique(pd.concat([user_artists['artistID'], user_taggedartists['artistID']]))\n","num_unique_artists = len(unique_artists)\n","print(f\"Number of unique artists: {num_unique_artists}\")\n","\n","# Count unique tags\n","unique_tags = pd.unique(user_taggedartists['tagID'])\n","num_unique_tags = len(unique_tags)\n","print(f\"Number of unique tags: {num_unique_tags}\")\n","\n","# Count listens_to relations\n","num_listens_to_relations = len(user_artists)\n","print(f\"Number of 'listens_to' relations: {num_listens_to_relations}\")\n","\n","# Count assigns_tag relations (unique user-tag pairs)\n","num_assigns_tag_relations = len(user_taggedartists[['userID', 'tagID']].drop_duplicates())\n","print(f\"Number of 'assigns_tag' relations: {num_assigns_tag_relations}\")\n","\n","# Count has_tag relations (unique artist-tag pairs)\n","num_has_tag_relations = len(user_taggedartists[['artistID', 'tagID']].drop_duplicates())\n","print(f\"Number of 'has_tag' relations: {num_has_tag_relations}\")"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Gy8Gv4RaObRr","executionInfo":{"status":"ok","timestamp":1701867802806,"user_tz":-480,"elapsed":4,"user":{"displayName":"Pengxin Wang","userId":"11146925416336724082"}}},"outputs":[],"source":["# Filter out tags that are not used\n","tags = tags[tags['tagID'].isin(unique_tags)]\n","\n","# Now 'tags' only contains tags that are actually used"]},{"cell_type":"markdown","metadata":{"id":"DllKuDjgObRs"},"source":["### Convert into required data format"]},{"cell_type":"markdown","metadata":{"id":"Ns0muW_XObRs"},"source":["**Step 1:** we need to re-index all entites and relations in the required format.\n","\n","**note:**  we do not have name for \"User\" Entities, so we just use the ID as its name.\n","\n","**Important:** index from 0!! index from 1 will cause error passing through embedding layer."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"a63UWO3fObRs","executionInfo":{"status":"ok","timestamp":1701867804643,"user_tz":-480,"elapsed":2,"user":{"displayName":"Pengxin Wang","userId":"11146925416336724082"}}},"outputs":[],"source":["# Create a DataFrame for users with the old 'userID' as the index\n","users = pd.DataFrame({'old_id': pd.unique(user_artists['userID']), 'type': 'user'})\n","users = users.reset_index()\n","users['new_id'] = users.index   # New unique ID\n","users['name'] = users['new_id'].astype(str)  # Use the new unique ID as the 'name'\n","\n","# Prepare the artists and tags DataFrames\n","artists = artists.reset_index()\n","artists['new_id'] = artists.index + len(users)\n","artists['name'] = artists['name'].astype(str)\n","\n","tags = tags.reset_index()\n","tags['new_id'] = tags.index + len(users) + len(artists)\n","tags['name'] = tags['tagValue']\n","\n","# Concatenate all entities into a single DataFrame with their new unique ID and names\n","all_entities = pd.concat([\n","    users[['name', 'new_id']],\n","    artists[['name', 'new_id']],\n","    tags[['name', 'new_id']]\n","])\n","# Now all_entities DataFrame will have unique IDs for all entities starting from 1\n","\n","# Given relation_ids dictionary\n","relation_ids = {\n","    \"listens_to\": 0,\n","    \"friends_with\": 1,\n","    \"assigns_tag\": 2,\n","    \"has_tag\": 3\n","}\n","\n","# Convert the dictionary to a pandas DataFrame\n","all_relations = pd.DataFrame(list(relation_ids.items()), columns=['relation', 'id'])\n","# Now we have a DataFrame with the relations and their corresponding IDs"]},{"cell_type":"markdown","metadata":{"id":"quc_5Y3bObRt"},"source":["**Step2:** We need to create entity-relation-entity triples in the required format."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"_9clZUHUObRt","executionInfo":{"status":"ok","timestamp":1701867806570,"user_tz":-480,"elapsed":2,"user":{"displayName":"Pengxin Wang","userId":"11146925416336724082"}}},"outputs":[],"source":["# Convert User-Artist relationships\n","user_artist_triples = user_artists[['userID', 'artistID']].copy()\n","user_artist_triples['relationID'] = relation_ids['listens_to']\n","\n","# We'll use the unique tags directly associated with artists for the \"has_tag\" relation\n","artist_tag_triples = user_taggedartists[['artistID', 'tagID']].drop_duplicates().copy()\n","artist_tag_triples['relationID'] = relation_ids['has_tag']\n","\n","# User-Tag assignment (ignoring the specific artistID)\n","user_tag_triples = user_taggedartists[['userID', 'tagID']].drop_duplicates().copy()\n","user_tag_triples['relationID'] = relation_ids['assigns_tag']\n","\n","# Convert User-Friends relationships\n","user_friends_triples = user_friends[['userID', 'friendID']].copy()\n","user_friends_triples['relationID'] = relation_ids['friends_with']\n","\n","# Create a mapping from old IDs to new IDs for each entity type\n","user_id_mapping = users.set_index('old_id')['new_id'].to_dict()\n","artist_id_mapping = artists.set_index('id')['new_id'].to_dict()\n","tag_id_mapping = tags.set_index('tagID')['new_id'].to_dict()\n","\n","# Update the IDs in the user-artist relationship DataFrame\n","user_artist_triples['userID'] = user_artist_triples['userID'].map(user_id_mapping)\n","user_artist_triples['artistID'] = user_artist_triples['artistID'].map(artist_id_mapping)\n","\n","# Update the IDs in the artist-tag relationship DataFrame\n","artist_tag_triples['artistID'] = artist_tag_triples['artistID'].map(artist_id_mapping)\n","artist_tag_triples['tagID'] = artist_tag_triples['tagID'].map(tag_id_mapping)\n","\n","# Update the IDs in the user-tag relationship DataFrame\n","user_tag_triples['userID'] = user_tag_triples['userID'].map(user_id_mapping)\n","user_tag_triples['tagID'] = user_tag_triples['tagID'].map(tag_id_mapping)\n","\n","# Update the IDs in the user-friends relationship DataFrame\n","user_friends_triples['userID'] = user_friends_triples['userID'].map(user_id_mapping)\n","user_friends_triples['friendID'] = user_friends_triples['friendID'].map(user_id_mapping)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"HgZUS9mQObRt","executionInfo":{"status":"ok","timestamp":1701868006962,"user_tz":-480,"elapsed":2843,"user":{"displayName":"Pengxin Wang","userId":"11146925416336724082"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d3f13ddc-b051-4b94-9318-13069b69b1bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/TransR/Data/Processed/lastfm\n"]}],"source":["%cd /content/drive/My Drive/TransR/Data/Processed/lastfm/\n","# Write to entity2id.txt\n","with open('./entity2id.txt', 'w+') as file:\n","    # First line is the number of entities\n","    file.write(f\"{len(all_entities)}\\n\")\n","    # Then write all entities and their new IDs\n","    for index, row in all_entities.iterrows():\n","        file.write(f\"{row['name']}\\t{row['new_id']}\\n\")\n","\n","# Write to relation2id.txt\n","with open('./relation2id.txt', 'w+') as file:\n","    # First line is the number of relations\n","    file.write(f\"{len(all_relations)}\\n\")\n","    # Then write all relations and their IDs\n","    for index, row in all_relations.iterrows():\n","        file.write(f\"{row['relation']}\\t{row['id']}\\n\")"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"oYcnIibxObRt","executionInfo":{"status":"ok","timestamp":1701868012163,"user_tz":-480,"elapsed":509,"user":{"displayName":"Pengxin Wang","userId":"11146925416336724082"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","import pandas as pd\n","\n","# First, rename the columns in each triple dataframe to have a consistent \"entity1\", \"entity2\", \"relationID\" format\n","user_artist_triples.rename(columns={'userID': 'entity1', 'artistID': 'entity2'}, inplace=True)\n","artist_tag_triples.rename(columns={'artistID': 'entity1', 'tagID': 'entity2'}, inplace=True)\n","user_tag_triples.rename(columns={'userID': 'entity1', 'tagID': 'entity2'}, inplace=True)\n","user_friends_triples.rename(columns={'userID': 'entity1', 'friendID': 'entity2'}, inplace=True)\n","\n","# Combine all updated triples into one DataFrame\n","all_triples = pd.concat([\n","    user_artist_triples[['entity1', 'entity2', 'relationID']],\n","    artist_tag_triples[['entity1', 'entity2', 'relationID']],\n","    user_tag_triples[['entity1', 'entity2', 'relationID']],\n","    user_friends_triples[['entity1', 'entity2', 'relationID']]\n","])\n","\n","# Remove any NaN values\n","all_triples.dropna(inplace=True)\n","all_triples = all_triples.astype(int)\n","\n","# Shuffle the DataFrame\n","all_triples = all_triples.sample(frac=1, random_state=7008).reset_index(drop=True)\n","\n","# Split the data into train, validation, and test sets\n","train, temp = train_test_split(all_triples, test_size=0.2, random_state=7008)\n","valid, test = train_test_split(temp, test_size=0.5, random_state=7008)\n","\n","# Save the data sets to txt files\n","def save_to_txt(df, file_path):\n","    with open(file_path, 'w+') as f:\n","        f.write(f\"{len(df)}\\n\")  # First line: number of triples\n","        df.to_csv(f, sep=' ', index=False, header=False)\n","\n","save_to_txt(train, './train2id.txt')\n","save_to_txt(valid, './valid2id.txt')\n","save_to_txt(test, './/test2id.txt')"]}],"metadata":{"kernelspec":{"display_name":"MATH7224","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}